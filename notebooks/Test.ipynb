{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c43213-1e24-49e0-bcc5-e424e0a4c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import os\n",
    "\n",
    "# Step 1: Define Chunk Size and Load Data in Chunks\n",
    "chunk_size = 50000  # Set the chunk size based on system memory\n",
    "file_path = \"fraudTrain.csv\"  # Replace with your dataset file path\n",
    "output_cleaned_data = \"test.csv\"  # For cleaned data storage\n",
    "\n",
    "# Initialize an empty list to collect chunks\n",
    "cleaned_chunks = []\n",
    "\n",
    "# Function to calculate distance\n",
    "def calculate_distance(row):\n",
    "    user_location = (row['lat'], row['long'])\n",
    "    merchant_location = (row['merch_lat'], row['merch_long'])\n",
    "    return geodesic(user_location, merchant_location).miles\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Step 2: Data Cleaning\n",
    "    # Parse datetime columns\n",
    "    chunk['trans_date_trans_time'] = pd.to_datetime(chunk['trans_date_trans_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    chunk['dob'] = pd.to_datetime(chunk['dob'], format='%Y-%m-%d')\n",
    "    chunk['unix_time'] = pd.to_datetime(chunk['unix_time'], unit='s')\n",
    "\n",
    "    # Create an age column\n",
    "    chunk['age'] = (pd.Timestamp.now() - chunk['dob']).dt.days // 365\n",
    "\n",
    "    # Calculate distance between user and merchant locations\n",
    "    chunk['distance'] = chunk.apply(calculate_distance, axis=1)\n",
    "\n",
    "    # Drop unnecessary columns to reduce memory usage\n",
    "    chunk = chunk.drop(['street', 'trans_num'], axis=1)\n",
    "\n",
    "    # Append cleaned chunk to the list\n",
    "    cleaned_chunks.append(chunk)\n",
    "\n",
    "# Combine all chunks into a single dataframe\n",
    "cleaned_data = pd.concat(cleaned_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b006d-4ab4-436f-a07f-ab1297fd8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data to a file for future use\n",
    "cleaned_data.to_csv(output_cleaned_data, index=False)\n",
    "print(\"Data cleaning completed and saved!\")\n",
    "\n",
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "# Distribution of transaction amounts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cleaned_data['amt'], bins=50, kde=True, color='blue')\n",
    "plt.title('Transaction Amount Distribution')\n",
    "plt.xlabel('Transaction Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcce13-5c99-422f-b14e-f425fd17061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud vs. Non-Fraud transaction counts\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='is_fraud', data=cleaned_data, palette='Set2')\n",
    "plt.title('Fraud vs Non-Fraud Transactions')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189a820-e888-405e-af75-3802398bf9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation = cleaned_data.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122aa175-bb71-444c-b32b-2b22fecca02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare Data for Machine Learning\n",
    "# Feature selection\n",
    "features = ['amt', 'age', 'city_pop', 'distance']\n",
    "X = cleaned_data[features]\n",
    "y = cleaned_data['is_fraud']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 5: Train a Machine Learning Model\n",
    "# Use Random Forest for fraud detection\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation Metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "# Step 7: Save the Model for Deployment\n",
    "import joblib\n",
    "model_file = \"fraud_detection_model.pkl\"\n",
    "joblib.dump(rf_model, model_file)\n",
    "print(f\"Model saved as {model_file}!\")\n",
    "\n",
    "# Step 8: Automate Pipeline for Large Dataset (if needed)\n",
    "# Define a function for automation\n",
    "def fraud_detection_pipeline(file_path, chunk_size=50000):\n",
    "    cleaned_chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Clean and process each chunk\n",
    "        chunk['trans_date_trans_time'] = pd.to_datetime(chunk['trans_date_trans_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        chunk['dob'] = pd.to_datetime(chunk['dob'], format='%Y-%m-%d')\n",
    "        chunk['unix_time'] = pd.to_datetime(chunk['unix_time'], unit='s')\n",
    "        chunk['age'] = (pd.Timestamp.now() - chunk['dob']).dt.days // 365\n",
    "        chunk['distance'] = chunk.apply(calculate_distance, axis=1)\n",
    "        chunk = chunk.drop(['street', 'trans_num'], axis=1)\n",
    "        cleaned_chunks.append(chunk)\n",
    "    \n",
    "    cleaned_data = pd.concat(cleaned_chunks, ignore_index=True)\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c9bc2-0387-471c-989a-f7bf685d62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model_pickle.obj','wb') as f:\n",
    "    pickle.dump(model_file,f)\n",
    "with open('model_pickle.obj','rb') as f:\n",
    "    mp=pickle.load(f)\n",
    "Age_prediction_lin2=mp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728cb3f-365b-46d5-abee-badb1aa8fd98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
